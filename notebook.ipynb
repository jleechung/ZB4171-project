{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataloader import *\n",
    "from evaluation import *\n",
    "import warnings\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load the data. Supply the path csv files containing:\n",
    "\n",
    "- raw counts\n",
    "- cell centroids\n",
    "\n",
    "Optionally, metadata can also be supplied, and a `label` corresponding to the cell type in the metadata can be provided for cell type labels to be used in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERFISH \n",
    "# counts_path = 'data/merfish/hypo_ani1_counts.csv'\n",
    "# centroids_path = 'data/merfish/hypo_ani1_cellcentroids.csv'\n",
    "# metadata_path = 'data/merfish/hypo_ani1_metadata.csv'\n",
    "\n",
    "# ST\n",
    "# counts_path = 'data/ST/ob_counts.csv'\n",
    "# centroids_path = 'data/ST/ob_centroids.csv'\n",
    "# metadata_path = 'data/ST/ob_metadata.csv'\n",
    "\n",
    "# ISH\n",
    "counts_path = 'data/ISH/drosophila_counts.csv'\n",
    "centroids_path = 'data/ISH/drosophila_centroids.csv'\n",
    "metadata_path = 'data/ISH/drosophila_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset = SpatialDataset(counts_path = counts_path, \n",
    "                      centroids_path = centroids_path,\n",
    "                      metadata_path = metadata_path, \n",
    "                      label = 'Cell_class', \n",
    "                      axes = ['Centroid_X', 'Centroid_Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, train_idx, val_idx = data_loader(\n",
    "    dset, batch_size=1, train_split = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(train_loader):\n",
    "    x = sample['cell_counts']\n",
    "    y = sample['neighbor_counts']\n",
    "    if idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full training on server \n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import yaml\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "import numpy as np\n",
    "from dataloader import *\n",
    "from shutil import copyfile\n",
    "\n",
    "LATENT_DIM=10\n",
    "HIDDEN_DIM1=128\n",
    "HIDDEN_DIM2=128\n",
    "HIDDEN_DIM3=128\n",
    "USE_CUDA=False\n",
    "LEARNING_RATE=1.0e-3\n",
    "NUM_EPOCHS=5\n",
    "TEST_FREQUENCY=2\n",
    "n_input = dset.n_features\n",
    "\n",
    "from models.nbcvae import *\n",
    "print('Training ZINB CVAE')\n",
    "vae = CVAE(\n",
    "    n_input=n_input,\n",
    "    z_dim=LATENT_DIM,\n",
    "    hidden_dim1=HIDDEN_DIM1,\n",
    "    hidden_dim2=HIDDEN_DIM2,\n",
    "    hidden_dim3=HIDDEN_DIM3,\n",
    "    use_cuda=USE_CUDA\n",
    ")\n",
    "\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "loss = Trace_ELBO()\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=loss)\n",
    "\n",
    "train_elbo = []\n",
    "val_elbo = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "        train_elbo.append(-total_epoch_loss_train)\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "        if epoch > 0 and epoch % TEST_FREQUENCY == 0:\n",
    "            # report test diagnostics\n",
    "            total_epoch_loss_test = evaluate(svi, val_loader, use_cuda=USE_CUDA)\n",
    "            val_elbo.append(-total_epoch_loss_test)\n",
    "            print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NB CVAE\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "CONFIG_FILE = 'checkpoints/NBCVAE_20211108-174537/config.yaml'\n",
    "with open(CONFIG_FILE) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "model_params = config['model_params']\n",
    "MODEL_NAME = model_params['model_name']\n",
    "LATENT_DIM = model_params['latent_dim']\n",
    "HIDDEN_DIM1 = model_params['hidden_dim1']\n",
    "HIDDEN_DIM2 = model_params['hidden_dim2']\n",
    "HIDDEN_DIM3 = model_params['hidden_dim3']\n",
    "\n",
    "# Data\n",
    "data_params = config['data_params']\n",
    "COUNTS_PATH = data_params['counts_path']\n",
    "CENTROIDS_PATH = data_params['centroids_path']\n",
    "METADATA_PATH = data_params['metadata_path']\n",
    "N_NEIGHBORS = data_params['n_neighbors']\n",
    "\n",
    "# Experiment parameters\n",
    "exp_params = config['exp_params']\n",
    "BATCH_SIZE = exp_params['batch_size']\n",
    "TRAIN_SPLIT = exp_params['train_split']\n",
    "LEARNING_RATE = exp_params['learning_rate']\n",
    "USE_CUDA = False\n",
    "NUM_EPOCHS = exp_params['num_epochs']\n",
    "TEST_FREQUENCY = exp_params['test_frequency']\n",
    "SAVE_FREQUENCY = exp_params['save_frequency']\n",
    "SAVE_DIR = exp_params['save_dir']\n",
    "\n",
    "n_input = dset.n_features\n",
    "n_class = dset.n_class\n",
    "\n",
    "if MODEL_NAME == 'ZINBVAE':\n",
    "    from models.vae import *\n",
    "    print('Training ZINB VAE')\n",
    "    vae = ZINBVAE(\n",
    "        n_input=n_input,\n",
    "        z_dim=LATENT_DIM,\n",
    "        hidden_dim1=HIDDEN_DIM1,\n",
    "        hidden_dim2=HIDDEN_DIM2,\n",
    "        hidden_dim3=HIDDEN_DIM3,\n",
    "        use_cuda=USE_CUDA\n",
    "    )\n",
    "\n",
    "elif MODEL_NAME == 'NBVAE':\n",
    "    from models.vae import *\n",
    "    print('Training NB VAE')\n",
    "    vae = NBVAE(\n",
    "        n_input=n_input,\n",
    "        z_dim=LATENT_DIM,\n",
    "        hidden_dim1=HIDDEN_DIM1,\n",
    "        hidden_dim2=HIDDEN_DIM2,\n",
    "        hidden_dim3=HIDDEN_DIM3,\n",
    "        use_cuda=USE_CUDA\n",
    "    )\n",
    "\n",
    "# conditional vae\n",
    "elif MODEL_NAME == 'CVAE':\n",
    "    from models.cvae import *\n",
    "    print('Training ZINB CVAE')\n",
    "    vae = CVAE(\n",
    "        n_input=n_input,\n",
    "        z_dim=LATENT_DIM,\n",
    "        hidden_dim1=HIDDEN_DIM1,\n",
    "        hidden_dim2=HIDDEN_DIM2,\n",
    "        hidden_dim3=HIDDEN_DIM3,\n",
    "        use_cuda=USE_CUDA\n",
    "    )\n",
    "\n",
    "elif MODEL_NAME == 'NBCVAE':\n",
    "    from models.nbcvae import *\n",
    "    print('Training NB CVAE')\n",
    "    vae = CVAE(\n",
    "        n_input=n_input,\n",
    "        z_dim=LATENT_DIM,\n",
    "        hidden_dim1=HIDDEN_DIM1,\n",
    "        hidden_dim2=HIDDEN_DIM2,\n",
    "        hidden_dim3=HIDDEN_DIM3,\n",
    "        use_cuda=USE_CUDA\n",
    "    )\n",
    "elif MODEL_NAME == 'LabelVAE':\n",
    "    from models.labelvae import *\n",
    "    print('Training LabelVAE')\n",
    "    vae = LabelVAE(\n",
    "        n_genes=n_input,\n",
    "        n_class=n_class,\n",
    "        z_dim=LATENT_DIM,\n",
    "        hidden_dim1=HIDDEN_DIM1,\n",
    "        hidden_dim2=HIDDEN_DIM2,\n",
    "        hidden_dim3=HIDDEN_DIM3,\n",
    "        use_cuda=USE_CUDA\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CVAE(\n",
       "  (prior): Encoder(\n",
       "    (fc1): Linear(in_features=84, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (mean_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (generation): Decoder(\n",
       "    (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (rate): Linear(in_features=128, out_features=84, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (recognition): Encoder(\n",
       "    (fc1): Linear(in_features=84, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (mean_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'checkpoints/NBCVAE_20211108-174537/model_epochs100.pt'\n",
    "vae.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output latent and reconstruction for cvae\n",
    "OUT_DIR = 'checkpoints/NBCVAE_20211108-174537'\n",
    "result = nbcvae_evaluation(vae, train_loader, out_dir = OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model weights\n",
    "weights = dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
    "index = 1\n",
    "lweights = weights[list(weights.keys())[index]].numpy()\n",
    "df = pd.DataFrame(lweights)\n",
    "df.to_csv('checkpoints/CVAE_20211107-200530/layer1_weights.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lat = get_latent(vae, 'LabelVAE', train_loader, \n",
    "                 file = 'checkpoints/LabelVAE_20211107-023946/latent.csv')\n",
    "rec = get_recon(vae, 'LabelVAE', train_loader, distribution = 'ZINB',\n",
    "                file = 'checkpoints/LabelVAE_20211107-023946/recon.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svi",
   "language": "python",
   "name": "svi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
